
########### GAN type ###########
gan_type: lsgan # which GAN type to train, e.g. "lsgan", "dcgan", "wgangp"

########### Network Hyperparams ###########
leakiness: 0.1
lr_g: 0.0001
lr_d1: 0.0001
lr_d2: 0.0001
beta1: 0.5
nz: 100
weight_decay: 0
num_epochs: 20000
ndf: 64
ngf: 64
batch_size: 16
image_size: 64
use_lsgan_loss: True
switch_loss_each_epoch: False
kernel_size: 4
use_one_sided_label_smoothing: False

########### Hardware & I/O ###########
workers: 2
ngpu: 1
num_epochs_before_gan_storage: 500
num_epochs_between_gan_storage: 50

########### Training Dataset ###########
metadata_path: setup/all_metadata.json
data:
  train:
    dataset_names:
      - inbreast
      - bcdr
      - cbis-ddsm
    roi_types:
      - mass
      - calcification
      - other
is_training_data_augmented: True

########### Preprocessing ###########
zoom_offset:  0.  # 0.2 # the higher, the more likely the patch is zoomed out. if 0, no offset. negative means, patch is rather zoomed in
zoom_spread: 0.  # 0.33 # the higher, the more variance in zooming. Must be greater or equal 0. with 0. being minimal variance.
ratio_spread:  0.  # 0.05 # coefficient for how much to spread the ratio between height and width. the higher, the more spread.
translation_spread: 0.  # 0.25 # the higher, the more variance in translation. Must be greater or equal 0. with 0. being minimal variance.
max_translation_offset: 0. # 0.33 # coefficient relative to the image size.

########### Pretraining of Dual Discriminator ###########
model_name: None # "cnn" # "swin_transformer"
pretrain_classifier: False
are_Ds_alternating_to_update_G: False
start_training_D2_after_epoch: 0
start_backprop_D2_into_G_after_epoch: 0

########### Conditional GAN Training ###########
conditioned_on: None
conditional: False
is_condition_binary: False
is_condition_categorical: False
split_birads_fours: False
added_noise_term: 0. # relevant for adding some random noise for the condition label of the conditional GAN.


